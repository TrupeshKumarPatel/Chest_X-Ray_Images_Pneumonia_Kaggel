{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.clear_all_output();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf')\n",
    "\n",
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29) \n",
      "[GCC 7.3.0]\n",
      "Version info.:  sys.version_info(major=3, minor=6, micro=10, releaselevel='final', serial=0)\n",
      "TensorFlow version:  2.1.0\n",
      "TensorFlow.Keras version :  2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import shutil\n",
    "import imgaug as aug\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mimg\n",
    "import imgaug.augmenters as iaa\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2\n",
    "# from keras import backend as K\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "os.environ['AUTOGRAPH_VERBOSITY'] = \"10\"\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.compat.v1.logging.set_verbosity(0)\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "print(\"Python version: \", sys.version)\n",
    "print(\"Version info.: \", sys.version_info)\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "print(\"TensorFlow.Keras version : \", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turn interactive plotting off\n",
    "# plt.ioff()\n",
    "# Set the seed for hash based operations in python\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Set the numpy seed\n",
    "np.random.seed(111)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "config.gpu_options.visible_device_list='0,1,2,3'\n",
    "# config.gpu_options.visible_device_list='0,1'\n",
    "\n",
    "# Set the random seed in tensorflow at graph level\n",
    "tf.compat.v1.set_random_seed(111)\n",
    "\n",
    "# Set the session in tensorflow\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "# Set the session in keras\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "# strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "print(\"#--#--\"*10)\n",
    "print('Number of devices: {}\\n'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "# Make the augmentation sequence deterministic\n",
    "aug.seed(111)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileTime = str(datetime.datetime.now().strftime(\"%m-%d-%Y-%H-%M\"))\n",
    "print(\"#--#--\"*10, \"\\nFile time: \", FileTime, \"\\n\\n\")\n",
    "\n",
    "# Define path to the data directory\n",
    "data_dir = Path('/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Data/new_chest_xray/')\n",
    "\n",
    "# Path to train directory (Fancy pathlib...no more os.path!!)\n",
    "train_dir = data_dir / 'train'\n",
    "\n",
    "# Path to validation directory\n",
    "val_dir = data_dir / 'val'\n",
    "\n",
    "# Path to test directory\n",
    "test_dir = data_dir / 'test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    train_list_ds = tf.data.Dataset.list_files(str(train_dir/\"*\"/\"*\"))\n",
    "    test_list_ds = tf.data.Dataset.list_files(str(test_dir/\"*\"/\"*\"))\n",
    "    val_list_ds = tf.data.Dataset.list_files(str(val_dir/\"*\"/\"*\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for f in train_list_ds.take(1):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = np.array(['NORMAL','PNEUMONIA'])\n",
    "def get_label(file_path):\n",
    "    label = None\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    print(parts)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == CLASS_NAMES\n",
    "\n",
    "def decode_img(img, IMG_WIDTH=299, IMG_HEIGHT=299):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    train_labeled_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    test_labeled_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    val_labeled_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# for image, label in labeled_ds.take(1):\n",
    "#     print(\"Image shape: \", image.numpy().shape)\n",
    "#     print(\"Label: \", label.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 32\n",
    "# BUFFER_SIZE = 1000\n",
    "IMG_SIZE = 299\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "nb_epochs = 1\n",
    "BUFFER_SIZE = 16\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with strategy.scope():\n",
    "#     train_batches = train_labeled_ds.repeat().cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "#     validation_batches = val_labeled_ds.repeat().batch(BATCH_SIZE)\n",
    "#     test_batches = test_labeled_ds.batch(BATCH_SIZE)\n",
    "\n",
    "train_batches = train_labeled_ds.repeat().batch(BATCH_SIZE).shuffle(BUFFER_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation_batches = val_labeled_ds.repeat().batch(BATCH_SIZE)\n",
    "test_batches = test_labeled_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_batch, label_batch = next(iter(train_batches))\n",
    "# show_batch(image_batch.numpy(), label_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = !ls $train_dir/*/*.jpeg | wc -l\n",
    "# test_size = !ls $test_dir/*/*.jpeg | wc -l\n",
    "# val_size = !ls $val_dir/*/*.jpeg | wc -l\n",
    "\n",
    "normal_cases_dir = train_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = train_dir / 'PNEUMONIA'\n",
    "train_size = len(list(normal_cases_dir.glob('*.jpeg'))) + len(list(pneumonia_cases_dir.glob('*.jpeg')))\n",
    "normal_cases_dir = test_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = test_dir / 'PNEUMONIA'\n",
    "test_size = len(list(normal_cases_dir.glob('*.jpeg'))) + len(list(pneumonia_cases_dir.glob('*.jpeg')))\n",
    "normal_cases_dir = val_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = val_dir / 'PNEUMONIA'\n",
    "val_size = len(list(normal_cases_dir.glob('*.jpeg'))) + len(list(pneumonia_cases_dir.glob('*.jpeg')))\n",
    "\n",
    "# train_size, test_size, val_size = int(train_size[0]), int(test_size[0]), int(val_size[0])\n",
    "\n",
    "print(\"#--#--\"*10,\"\\ntraining: {},\\nvalidation: {},\\ntest: {}\".format(train_size, \n",
    "                                                                        val_size, \n",
    "                                                                        test_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_validation_concat = train_batches.concatenate(validation_batches)\n",
    "concat_step = (train_size + val_size)//BATCH_SIZE\n",
    "print(concat_step)\n",
    "l = []\n",
    "for _, y in train_validation_concat.take(concat_step+2):\n",
    "    for i in y:\n",
    "        l.append(i)\n",
    "    \n",
    "y_concat = tf.convert_to_tensor(np.asarray(l, dtype=np.bool),  dtype=tf.bool)\n",
    "print(y_concat.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for image_batch, label_batch in train_batches.take(1):\n",
    "    pass\n",
    "\n",
    "print(image_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_inventory(inventory_name, dct):\n",
    "    print('%s :' %(inventory_name))\n",
    "    for item, amount in dct.items():  # dct.iteritems() in Python 2\n",
    "        print('%15s : %s' % (item, amount))\n",
    "        \n",
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\nLearning rate for epoch {} is {}\\n'.format(epoch + 1, model.optimizer.lr.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = learning_rate\n",
    "exp_decay = tf.compat.v1.train.exponential_decay(starter_learning_rate,\n",
    "                                                 global_step, 100000, \n",
    "                                                 0.96, staircase=True)\n",
    "\n",
    "callbacks = [\n",
    "#     tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ReduceLROnPlateau(monitor='val_auc', factor=0.3, patience=5, verbose=1, mode='max', min_delta=0.0001),\n",
    "    # ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max'),\n",
    "    # EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, mode='min', restore_best_weights=True),\n",
    "\n",
    "    # EarlyStopping(  monitor='val_loss', min_delta=1e-3, patience=5, verbose=1,\n",
    "    #                 mode='auto', baseline=None, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/best_model_todate_python/'+FileTime+'/',\n",
    "                    monitor='val_auc', verbose=1, mode='max',\n",
    "                    save_best_only=True, save_weights_only=True),\n",
    "#     tf.keras.callbacks.LearningRateScheduler(exp_decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of training steps\n",
    "nb_train_steps = train_size//BATCH_SIZE\n",
    "\n",
    "# Define the number of validation steps\n",
    "nb_valid_steps = val_size//BATCH_SIZE\n",
    "\n",
    "# Define the number of validation steps\n",
    "nb_test_steps = test_size//BATCH_SIZE\n",
    "\n",
    "print(\"#--#--\"*10,\"\\n\\nNumber of training and validation steps: {} and {}\".format(nb_train_steps, \n",
    "                                                                                  nb_valid_steps))\n",
    "\n",
    "print(\"#--#--\"*10,  \"\\n BATCH_SIZE_PER_REPLICA = \", BATCH_SIZE_PER_REPLICA,\n",
    "                    \"\\n BATCH_SIZE = \", BATCH_SIZE,\n",
    "                    \"\\n EPOCHS = \", nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "with strategy.scope():\n",
    "    base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')\n",
    "    base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with strategy.scope():\n",
    "    feature_batch = base_model(image_batch)\n",
    "#     feature_batch = base_model.predict(train_validation_concat, verbose=1)\n",
    "\n",
    "print(feature_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    inputs = base_model.output\n",
    "    flat = Flatten(name='flatten')(inputs)\n",
    "    dense_1 = Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), name='fc1')(flat)\n",
    "    drop_1 = Dropout(0.7, name='dropout1')(dense_1)\n",
    "    dense_2 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), name='fc2')(drop_1)\n",
    "    drop_2 = Dropout(0.5, name='dropout2')(dense_2)\n",
    "    predict = Dense(2, activation='softmax', name='fc3')(drop_2)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predict)\n",
    "    model._name = \"FrozenModel\"\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=learning_rate, amsgrad=True, clipnorm=1., decay=0.004)\n",
    "#     opt = tf.keras.optimizers.Nadam()\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=opt, metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"#--#--\"*10)\n",
    "print_inventory(\"Optimizer\", opt.get_config())\n",
    "# print(\"#--#--\"*10)\n",
    "# print_inventory(\"Early Stopping\", opt.get_config())\n",
    "# print(\"#--#--\"*10)\n",
    "# print_inventory(\"Reduce LR On Plateau\", opt.get_config())\n",
    "\n",
    "print('\\nReduce LR On Plateau :\\n%15s : %s,\\n%15s : %s,\\n%15s : %s,\\n%15s : %s'%('Monitor', callbacks[0].monitor,\n",
    "                                                                                 'Factor', callbacks[0].factor,\n",
    "                                                                                 'Mode', callbacks[0].mode,\n",
    "                                                                                 'Patience', callbacks[0].patience) )\n",
    "\n",
    "# print('\\n Early Stopping:\\n%15s : %s,\\n%15s : %s,\\n%15s : %s' %('Monitor', callbacks[1].monitor,\n",
    "#                                                                 'MinDelta', callbacks[1].min_delta,\n",
    "#                                                                 'patience', callbacks[1].patience) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    history = model.fit(train_batches, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
    "                        validation_data=validation_batches, validation_steps=nb_valid_steps,\n",
    "                        callbacks=callbacks, verbose=1)#,[chkpt, PrintLR()],\n",
    "                        # class_weight={0:1.0, 1:0.4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelPath = './xray-best-model/best_model/best_model_'+datetime.datetime.now().strftime(\"%m-%d-%Y-%I-%M\")+'.h5'\n",
    "weightPath = '/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/xray-best-model/best_model/best_model_para-tune_'+FileTime+'.hdf5'\n",
    "# model.save(modelPath)\n",
    "model.save_weights(weightPath)\n",
    "# print(\"modelPath: \", modelPath)\n",
    "print(\"#--#--\"*10,\"\\n\\nweightPath: \", weightPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    base_model_reLoad = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,\n",
    "                                                          include_top=False,\n",
    "                                                          weights='imagenet')\n",
    "    for layer in base_model_reLoad.layers:\n",
    "        base_model_reLoad.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_reLoad.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    inputs = base_model_reLoad.output\n",
    "\n",
    "    x = Flatten(name='flatten')(inputs)\n",
    "    x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.7, name='dropout1')(x)\n",
    "    x = Dense(512, activation='relu', name='fc2')(x)\n",
    "    x = Dropout(0.5, name='dropout2')(x)\n",
    "    x = Dense(2, activation='softmax', name='fc3')(x)\n",
    "    \n",
    "    model_reLoad = Model(inputs=base_model_reLoad.input, outputs=x)\n",
    "    model_reLoad._name = \"UnfrozenModel\"\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=learning_rate, amsgrad=True, clipnorm=1.)\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "    model_reLoad.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=opt, metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reLoad.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reLoad.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model_reLoad.load_weights(weightPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_new_model(return_dict):\n",
    "with strategy.scope():\n",
    "    history_reLoad = model_reLoad.fit(train_batches, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
    "                                      validation_data=validation_batches, validation_steps=nb_valid_steps,\n",
    "                                      callbacks=callbacks, verbose=1)#,[chkpt, PrintLR()],\n",
    "                                    # class_weight={0:1.0, 1:0.4})\n",
    "#     return_dict['history'] = history_reLoad\n",
    "#     print(return_dict.values())\n",
    "#     return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_reLoad = run_new_model()\n",
    "# import multiprocessing\n",
    "# manager = multiprocessing.Manager()\n",
    "# return_dict = manager.dict()\n",
    "# process_eval = multiprocessing.Process(target=run_new_model, args=(return_dict,))\n",
    "# process_eval.start()\n",
    "# process_eval.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(return_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#--#--\"*10,\"\\n\\nTraining Completed \\n\\n\")\n",
    "print(\"history items : \", history_reLoad.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = history_reLoad\n",
    "fig = plt.figure()\n",
    "plt.plot(h.history['accuracy'])\n",
    "plt.plot(h.history['val_accuracy'])\n",
    "plt.plot(np.argmax(h.history[\"accuracy\"]),\n",
    "         np.max(h.history[\"val_accuracy\"]),\n",
    "         marker=\"x\", color=\"b\", label=\"best model\")\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train-accuracy', 'val-accuracy'], loc='upper left')\n",
    "#plt.savefig(\"/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/python_acc.png\", format='png')\n",
    "# plt.savefig('/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/plots/acc/python_acc-'+FileTime+'.png', format='png')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = history_reLoad\n",
    "fig = plt.figure()\n",
    "plt.plot(h.history['loss'])\n",
    "plt.plot(h.history['val_loss'])\n",
    "plt.plot(np.argmin(h.history[\"loss\"]),\n",
    "         np.min(h.history[\"val_loss\"]),\n",
    "         marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train-loss', 'val-loss'], loc='upper left')\n",
    "# plt.savefig('/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/plots/loss/python_loss-'+FileTime+'.png', format='png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = history_reLoad\n",
    "fig = plt.figure()\n",
    "plt.plot(h.history['auc_1'])\n",
    "plt.plot(h.history['val_auc_1'])\n",
    "plt.plot(np.argmax(h.history[\"auc_1\"]),\n",
    "         np.max(h.history[\"val_auc_1\"]),\n",
    "         marker=\"x\", color=\"b\", label=\"best model\")\n",
    "plt.title('model AUC (Area under the curve)')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train-AUC', 'val-AUC'], loc='upper left')\n",
    "#plt.savefig(\"/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/python_acc.png\", format='png')\n",
    "# plt.savefig('/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/plots/auc/python_auc-'+FileTime+'.png', format='png')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelPath = './xray-best-model/best_model/best_model_'+datetime.datetime.now().strftime(\"%m-%d-%Y-%I-%M\")+'.h5'\n",
    "weightPath = '/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/xray-best-model/best_model/best_model_para-tune_'+FileTime+'.hdf5'\n",
    "# model.save(modelPath)\n",
    "model_reLoad.save_weights(weightPath)\n",
    "# print(\"modelPath: \", modelPath)\n",
    "print(\"#--#--\"*10,\"\\n\\nweightPath: \", weightPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model_reLoad.load_weights(weightPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test dataset\n",
    "with strategy.scope():\n",
    "    test_loss, test_score, test_auc = model_reLoad.evaluate(test_batches)\n",
    "print(\"Loss on test set: \", test_loss)\n",
    "print(\"Accuracy on test set: \", test_score)\n",
    "print(\"AUC on test set: \", test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_test_labels=[]\n",
    "orig_test_labels=[]\n",
    "for X_sample, Y_sample in test_batches.take(4):\n",
    "    print(X_sample.shape, Y_sample.shape)\n",
    "#     with strategy.scope():\n",
    "    pred = model_reLoad.predict(X_sample)\n",
    "    for pY, oY in zip(pred, Y_sample):\n",
    "        pred_test_labels.append(pY)\n",
    "        orig_test_labels.append(oY)\n",
    "        \n",
    "pred_test_labels = np.argmax(pred_test_labels, axis=-1)\n",
    "orig_test_labels = np.argmax(orig_test_labels, axis=-1)\n",
    "\n",
    "print(\"#--#--\"*10,\"\\n\\n\",orig_test_labels.shape)\n",
    "print(pred_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrix\n",
    "cm  = confusion_matrix(orig_test_labels, pred_test_labels)\n",
    "fig = plt.figure()\n",
    "plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\n",
    "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "#plt.savefig(\"/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/python_confusion-mat.png\", format='png')\n",
    "# plt.savefig('/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/plots/confusion-mat/python_confusion-mat-'+FileTime+'.png', format='png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Calculate Precision and Recall\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = (2*precision*recall/(precision+recall))\n",
    "print(\"#--#--\"*10,\"\\n\\nRecall of the model is {:.2f}\".format(recall))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision))\n",
    "print(\"F1-score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to the data directory\n",
    "tes_data_dir = Path('/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Data/chest_xray/chest_xray/')\n",
    "\n",
    "# Path to test directory\n",
    "test_dir_2 = tes_data_dir / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    test_2_list_ds = tf.data.Dataset.list_files(str(test_dir_2/\"*\"/\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    test_2_labeled_ds = test_2_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2_batches = test_2_labeled_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_labels=[]\n",
    "orig_test_labels=[]\n",
    "for X_sample, Y_sample in test_2_batches.take(4):\n",
    "    print(X_sample.shape, Y_sample.shape)\n",
    "#     with strategy.scope():\n",
    "    pred = model_reLoad.predict(X_sample)\n",
    "    for pY, oY in zip(pred, Y_sample):\n",
    "        pred_test_labels.append(pY)\n",
    "        orig_test_labels.append(oY)\n",
    "        \n",
    "pred_test_labels = np.argmax(pred_test_labels, axis=-1)\n",
    "orig_test_labels = np.argmax(orig_test_labels, axis=-1)\n",
    "\n",
    "print(\"#--#--\"*10,\"\\n\\n\",orig_test_labels.shape)\n",
    "print(pred_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrix\n",
    "cm  = confusion_matrix(orig_test_labels, pred_test_labels)\n",
    "fig = plt.figure()\n",
    "plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\n",
    "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "#plt.savefig(\"/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/python_confusion-mat.png\", format='png')\n",
    "# plt.savefig('/data/user/tr27p/Courses/CS765-DeepLearning/FinalProject/Chest_X-Ray_Images_Pneumonia/Python/plots/confusion-mat/python_confusion-mat-'+FileTime+'.png', format='png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Calculate Precision and Recall\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "\n",
    "print(\"#--#--\"*10,\"\\n\\nRecall of the model is {:.2f}\".format(recall))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision))\n",
    "print(\"F1-score: {}\".format(2*precision*recall/(precision+recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
